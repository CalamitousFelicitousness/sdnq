#!/usr/bin/env python3
"""Quantize and save a full diffusers pipeline using per-component SDNQConfig.

Consumes the JSON pipeline config generated by analyze_quantization_sensitivity.py
(with --auto-config), or falls back to a uniform config from CLI args.

Each component is loaded, quantized, and saved individually to keep peak memory low.

Example usage:
    # Using generated per-component config
    python scripts/quantize_pipeline.py \
        --model-id Tongyi-MAI/Z-Image \
        --output-path /path/to/quantized \
        --pipeline-config quant_sensitivity_report_pipeline_config.json

    # Uniform config for all components
    python scripts/quantize_pipeline.py \
        --model-id Tongyi-MAI/Z-Image \
        --output-path /path/to/quantized \
        --weights-dtype int8
"""

import argparse
import json
import os
import shutil
import sys
from pathlib import Path

import torch

# Allow importing from the scripts directory
sys.path.insert(0, str(Path(__file__).parent))
from analyze_quantization_sensitivity import discover_components, resolve_class


def parse_args():
    p = argparse.ArgumentParser(description="Quantize a diffusers pipeline per-component")
    p.add_argument("--model-id", required=True,
                    help="HuggingFace model ID or local path")
    p.add_argument("--output-path", required=True,
                    help="Output directory for the quantized pipeline")
    p.add_argument("--pipeline-config", default=None,
                    help="JSON config from analyze script (per-component SDNQConfig params)")
    p.add_argument("--cache-dir", default="/home/ohiom/database/models/huggingface",
                    help="HuggingFace cache directory")
    p.add_argument("--device", default="cpu",
                    help="Device for quantization computation")
    p.add_argument("--max-shard-size", default="5GB",
                    help="Max shard size for saved model files")
    p.add_argument("--components", nargs="+", default=None,
                    help="Subset of components to quantize (subfolder names)")
    # Uniform fallback args (when no --pipeline-config)
    p.add_argument("--weights-dtype", default="int8",
                    help="Quantization dtype (used when no --pipeline-config)")
    p.add_argument("--group-size", type=int, default=0,
                    help="Group size (used when no --pipeline-config)")
    p.add_argument("--use-svd", action="store_true",
                    help="Enable SVD compression (used when no --pipeline-config)")
    p.add_argument("--svd-rank", type=int, default=32,
                    help="SVD rank (used when no --pipeline-config)")
    p.add_argument("--use-quantized-matmul", action="store_true",
                    help="Enable quantized matmul (used when no --pipeline-config)")
    return p.parse_args()


def load_pipeline_config(config_path):
    """Load per-component config from JSON file."""
    with open(config_path, encoding="utf-8") as f:
        return json.load(f)


def build_default_config(args):
    """Build a default config dict from uniform CLI args."""
    config = {
        "weights_dtype": args.weights_dtype,
        "group_size": args.group_size,
        "use_quantized_matmul": args.use_quantized_matmul,
    }
    if args.use_svd:
        config["use_svd"] = True
        config["svd_rank"] = args.svd_rank
    return config


def _extract_quant_kwargs(config_kwargs):
    """Extract kwargs compatible with sdnq_post_load_quant from a config dict."""
    valid_keys = {
        "weights_dtype", "group_size", "svd_rank", "svd_steps",
        "use_svd", "use_quantized_matmul", "dequantize_fp32",
        "modules_to_not_convert", "modules_dtype_dict", "modules_svd_dict",
        "modules_group_size_dict",
    }
    return {k: v for k, v in config_kwargs.items() if k in valid_keys}


def resolve_model_path(model_id, cache_dir):
    """Resolve the local cache path for a model ID."""
    local_path = Path(model_id)
    if local_path.is_dir():
        return str(local_path)

    try:
        from huggingface_hub import snapshot_download
        return snapshot_download(
            model_id, cache_dir=cache_dir, local_files_only=True,
        )
    except Exception:
        from huggingface_hub import snapshot_download
        print(f"Downloading {model_id}...")
        return snapshot_download(model_id, cache_dir=cache_dir)


def copy_pipeline_metadata(model_id, cache_dir, output_path):
    """Copy non-quantizable pipeline components (schedulers, tokenizers, etc.)."""
    # Components that have no quantizable weights
    skip_types = {
        "FlowMatchEulerDiscreteScheduler", "EulerDiscreteScheduler",
        "DDIMScheduler", "PNDMScheduler", "DPMSolverMultistepScheduler",
        "LMSDiscreteScheduler", "UniPCMultistepScheduler",
        "Qwen2Tokenizer", "CLIPTokenizer", "T5Tokenizer",
        "T5TokenizerFast", "CLIPTokenizerFast", "LlamaTokenizer",
        "LlamaTokenizerFast", "PreTrainedTokenizerFast",
        "CLIPImageProcessor", "CLIPFeatureExtractor",
    }

    local_path = resolve_model_path(model_id, cache_dir)

    model_index_path = os.path.join(local_path, "model_index.json")
    if not os.path.exists(model_index_path):
        print("WARNING: model_index.json not found. Skipping metadata copy.")
        return

    with open(model_index_path, encoding="utf-8") as f:
        index = json.load(f)

    # Copy model_index.json
    shutil.copy2(model_index_path, os.path.join(output_path, "model_index.json"))
    print("  Copied model_index.json")

    # Copy non-quantizable component subfolders
    for key, value in index.items():
        if not isinstance(value, list) or len(value) < 2:
            continue
        class_name = value[1]
        if class_name not in skip_types:
            continue
        src_dir = os.path.join(local_path, key)
        dst_dir = os.path.join(output_path, key)
        if os.path.isdir(src_dir):
            if os.path.exists(dst_dir):
                shutil.rmtree(dst_dir)
            shutil.copytree(src_dir, dst_dir)
            print(f"  Copied {key}/ ({class_name})")
        else:
            print(f"  WARNING: {key}/ not found at {src_dir}, skipping")


def quantize_component(model_id, subfolder, class_name, config_kwargs,
                       output_path, cache_dir, device, max_shard_size):
    """Load, quantize, and save a single model component."""
    from sdnq import SDNQConfig, save_sdnq_model
    from sdnq.quantizer import sdnq_post_load_quant

    print(f"\nQuantizing {subfolder} ({class_name})...")
    cls = resolve_class(class_name)
    model = cls.from_pretrained(
        model_id, subfolder=subfolder,
        torch_dtype=torch.bfloat16, cache_dir=cache_dir,
    )
    model = model.to(device)

    quant_kwargs = _extract_quant_kwargs(config_kwargs)
    model = sdnq_post_load_quant(model, add_skip_keys=True, **quant_kwargs)

    sdnq_config = SDNQConfig(**{k: v for k, v in config_kwargs.items()
                                 if k not in ("use_quantized_matmul",)
                                 or k == "use_quantized_matmul"})

    component_path = os.path.join(output_path, subfolder)
    os.makedirs(component_path, exist_ok=True)
    save_sdnq_model(model, component_path, max_shard_size=max_shard_size,
                     is_pipeline=False, sdnq_config=sdnq_config)
    print(f"  Saved to {component_path}")

    del model
    if torch.cuda.is_available():
        torch.cuda.empty_cache()


def main():
    args = parse_args()

    # 1. Discover components
    components = discover_components(args.model_id, args.cache_dir)
    if components is None:
        print("ERROR: Could not discover pipeline components from model_index.json.")
        print("       Ensure the model is a diffusers pipeline with model_index.json.")
        sys.exit(1)

    print(f"Discovered {len(components)} quantizable components:")
    for c in components:
        print(f"  {c}")

    # 2. Load per-component configs
    if args.pipeline_config:
        per_comp_config = load_pipeline_config(args.pipeline_config)
        print(f"\nLoaded pipeline config from {args.pipeline_config}")
        for subfolder, cfg in per_comp_config.items():
            print(f"  {subfolder}: weights_dtype={cfg.get('weights_dtype', 'int8')}, "
                  f"group_size={cfg.get('group_size', 0)}")
    else:
        per_comp_config = {}

    default_config = build_default_config(args)

    # 3. Filter to requested components
    if args.components:
        requested = set(args.components)
        components = [c for c in components if c.split(":")[0] in requested]
        if not components:
            print(f"ERROR: No matching components found for {args.components}")
            sys.exit(1)
        print(f"\nFiltered to {len(components)} components: {[c.split(':')[0] for c in components]}")

    # 4. Copy non-model pipeline files
    os.makedirs(args.output_path, exist_ok=True)
    print(f"\nCopying pipeline metadata to {args.output_path}...")
    copy_pipeline_metadata(args.model_id, args.cache_dir, args.output_path)

    # 5. Quantize each component
    for comp_spec in components:
        subfolder, class_name = comp_spec.split(":", 1)
        config = per_comp_config.get(subfolder, default_config)
        quantize_component(
            args.model_id, subfolder, class_name,
            config, args.output_path, args.cache_dir,
            args.device, args.max_shard_size,
        )

    print(f"\nPipeline quantization complete. Output: {args.output_path}")


if __name__ == "__main__":
    main()
